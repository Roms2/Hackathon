{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1bdeb54b-d998-4720-a9e7-4f7152aeca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, recall_score, confusion_matrix, balanced_accuracy_score, \n",
    "                             average_precision_score, f1_score, roc_auc_score, classification_report, precision_recall_curve)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler,MultiLabelBinarizer\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c68b43-08bb-4dbf-b3bd-9cf758315c81",
   "metadata": {},
   "source": [
    "## Lecture du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9cce2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kddcup.names\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Ignorer la première ligne (qui contient les classes d'attaques)\n",
    "lines = lines[1:]\n",
    "\n",
    "# Extraire uniquement les noms des colonnes avant les `:` et supprimer les espaces\n",
    "columns = [line.split(\":\")[0].strip() for line in lines]\n",
    "\n",
    "# Ajouter la colonne cible \"label\" (classification)\n",
    "columns.append(\"label\")\n",
    "\n",
    "# Charger le fichier de données avec les colonnes extraites\n",
    "data = pd.read_csv(\"kddcup.data.corrected\", sep=\",\", header=None, names=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration  src_bytes  dst_bytes  land  wrong_fragment  urgent  hot  \\\n",
      "0         0        215      45076     0               0       0    0   \n",
      "1         0        162       4528     0               0       0    0   \n",
      "2         0        236       1228     0               0       0    0   \n",
      "3         0        233       2032     0               0       0    0   \n",
      "4         0        239        486     0               0       0    0   \n",
      "\n",
      "   num_failed_logins  logged_in  num_compromised  ...  flag_REJ  flag_RSTO  \\\n",
      "0                  0          1                0  ...     False      False   \n",
      "1                  0          1                0  ...     False      False   \n",
      "2                  0          1                0  ...     False      False   \n",
      "3                  0          1                0  ...     False      False   \n",
      "4                  0          1                0  ...     False      False   \n",
      "\n",
      "   flag_RSTOS0  flag_RSTR  flag_S0  flag_S1  flag_S2  flag_S3  flag_SF  \\\n",
      "0        False      False    False    False    False    False     True   \n",
      "1        False      False    False    False    False    False     True   \n",
      "2        False      False    False    False    False    False     True   \n",
      "3        False      False    False    False    False    False     True   \n",
      "4        False      False    False    False    False    False     True   \n",
      "\n",
      "   flag_SH  \n",
      "0    False  \n",
      "1    False  \n",
      "2    False  \n",
      "3    False  \n",
      "4    False  \n",
      "\n",
      "[5 rows x 122 columns]\n"
     ]
    }
   ],
   "source": [
    "data = data.iloc[:489843]\n",
    "\n",
    "# Séparer les caractéristiques et la cible\n",
    "X = data.drop('label', axis=1)\n",
    "Y = data['label']\n",
    "\n",
    "# Appliquer One-Hot Encoding sur les colonnes catégorielles\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Afficher les premières lignes du dataset après One-Hot Encoding\n",
    "print(X_encoded.head())\n",
    "\n",
    "# Appliquer Isolation Forest sur les données encodées\n",
    "IF = IsolationForest(n_estimators=100, contamination=0.002, random_state=1, n_jobs=-1)\n",
    "outliers_if = IF.fit_predict(X_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a24914d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant encodage - X.shape: (4898431, 41)\n",
      "Avant encodage - Y.shape: (4898431,)\n",
      "Après encodage - X_encoded.shape: (4898431, 122)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vérifier les dimensions avant encodage\n",
    "X = data.drop('label', axis=1)\n",
    "Y = data['label']\n",
    "\n",
    "print(f\"Avant encodage - X.shape: {X.shape}\")\n",
    "print(f\"Avant encodage - Y.shape: {Y.shape}\")\n",
    "\n",
    "# Appliquer One-Hot Encoding sur les colonnes catégorielles\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Vérifier les dimensions après encodage\n",
    "print(f\"Après encodage - X_encoded.shape: {X_encoded.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a8cf71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes détectées: ['back.' 'buffer_overflow.' 'ftp_write.' 'guess_passwd.' 'imap.'\n",
      " 'ipsweep.' 'land.' 'loadmodule.' 'multihop.' 'neptune.' 'nmap.' 'normal.'\n",
      " 'perl.' 'phf.' 'pod.' 'portsweep.' 'rootkit.' 'satan.' 'smurf.' 'spy.'\n",
      " 'teardrop.' 'warezclient.' 'warezmaster.']\n",
      "Aperçu des données encodées:\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "   back.  buffer_overflow.  ftp_write.  guess_passwd.  imap.  ipsweep.  land.  \\\n",
      "0      0                 0           0              0      0         0      0   \n",
      "1      0                 0           0              0      0         0      0   \n",
      "2      0                 0           0              0      0         0      0   \n",
      "3      0                 0           0              0      0         0      0   \n",
      "4      0                 0           0              0      0         0      0   \n",
      "\n",
      "   loadmodule.  multihop.  neptune.  ...  phf.  pod.  portsweep.  rootkit.  \\\n",
      "0            0          0         0  ...     0     0           0         0   \n",
      "1            0          0         0  ...     0     0           0         0   \n",
      "2            0          0         0  ...     0     0           0         0   \n",
      "3            0          0         0  ...     0     0           0         0   \n",
      "4            0          0         0  ...     0     0           0         0   \n",
      "\n",
      "   satan.  smurf.  spy.  teardrop.  warezclient.  warezmaster.  \n",
      "0       0       0     0          0             0             0  \n",
      "1       0       0     0          0             0             0  \n",
      "2       0       0     0          0             0             0  \n",
      "3       0       0     0          0             0             0  \n",
      "4       0       0     0          0             0             0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1️⃣ Transformation des labels en une liste de labels (si plusieurs étiquettes par ligne)\n",
    "data['label'] = data['label'].apply(lambda x: x.split(','))  # Si plusieurs labels sont séparés par des virgules\n",
    "\n",
    "# 2️⃣ Encodage avec MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y_encoded = mlb.fit_transform(data['label'])\n",
    "\n",
    "# 3️⃣ Affichage des nouvelles colonnes\n",
    "print(\"Classes détectées:\", mlb.classes_)\n",
    "print(\"Aperçu des données encodées:\")\n",
    "print(Y_encoded[:5])  # Afficher les 5 premières lignes\n",
    "\n",
    "# 4️⃣ (Optionnel) Transformer en DataFrame pour voir les colonnes\n",
    "Y_encoded_df = pd.DataFrame(Y_encoded, columns=mlb.classes_)\n",
    "print(Y_encoded_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6424946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = Y_encoded_df.sum()\n",
    "# Définir un seuil (ex: classes avec moins de 5 occurrences seront regroupées)\n",
    "min_samples = 5\n",
    "rare_classes = class_counts[class_counts < min_samples].index\n",
    "\n",
    "# Fusionner ces classes en une seule colonne \"autre\"\n",
    "Y_encoded_df['autre'] = Y_encoded_df[rare_classes].sum(axis=1)\n",
    "\n",
    "# Supprimer les colonnes des classes rares\n",
    "Y_encoded_df = Y_encoded_df.drop(columns=rare_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85007dc9-0475-4a60-b554-a8d952d04da3",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue;\">Approches supervisées</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e12ab28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models={\n",
    "       'RF' : RandomForestClassifier(n_estimators=50, random_state = 1, n_jobs=-1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "38739f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(Ytest,Pred,Prob):\n",
    "    ba=balanced_accuracy_score(Ytest,Pred)\n",
    "    f1=f1_score(Ytest,Pred)\n",
    "    ap=average_precision_score(Ytest,Prob)\n",
    "    print('Matrice de confusion') \n",
    "    print(confusion_matrix(Ytest,Pred))\n",
    "    print('Balanced Accuracy : %.3f' %ba)    \n",
    "    print('F1 Score : %.3f' %f1)\n",
    "    print('Average precision score : %.3f' %ap)\n",
    "    return ba,f1,ap\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "767149d8-a242-4d4a-87dd-d064c1422e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_approach(Xtrain,Ytrain,Xtest,Ytest,model):\n",
    "    model.fit(Xtrain,Ytrain)\n",
    "    Pred=model.predict(Xtest)\n",
    "    Prob=model.predict_proba(Xtest)[:,1]\n",
    "\n",
    "    best_f1 = f1_score(Ytest,Pred)\n",
    "    best_pred = Pred\n",
    "\n",
    "    for i in np.arange(1.0, 0.0, -0.01):\n",
    "        Pred_LR_new=Prob>=i\n",
    "        test_f1 = f1_score(Ytest,Pred_LR_new)\n",
    "        if test_f1 > best_f1 :\n",
    "            best_f1 = test_f1\n",
    "            best_pred = Pred_LR_new\n",
    "\n",
    "    \n",
    "    return best_pred, Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33f1c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tomek(X,Y):\n",
    "    tl = TomekLinks(n_jobs=-1)\n",
    "    X_tomek, y_Tomek = tl.fit_resample(X, Y)\n",
    "    return X_tomek, y_Tomek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93915b7a-14df-4f36-8802-1db81210859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampling_approach(Xtrain,Ytrain,Xtest,Ytest,model):\n",
    "    X_under, Y_under=Tomek(Xtrain,Ytrain)\n",
    "    Pred, Prob=original_approach(X_under, Y_under,Xtest,Ytest,model)\n",
    "    return Pred, Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b336424e-a7d2-4596-b025-d884b91ef6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Smote(X,Y):\n",
    "    sm=SMOTE(k_neighbors=5,random_state=1)\n",
    "    X_smote, y_smote = sm.fit_resample(X, Y)\n",
    "    return X_smote, y_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1eccdab3-ff2f-46a6-8fb3-ba19d2825836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling_approach(Xtrain,Ytrain,Xtest,Ytest,model):\n",
    "    X_over, Y_over=Smote(Xtrain,Ytrain)\n",
    "    Pred, Prob=original_approach(X_over, Y_over,Xtest,Ytest,model)\n",
    "    return Pred, Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a77070eb-7b9f-46c4-8ccd-1fb00ae4e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing_approach(Xtrain,Ytrain,Xtest,Ytest,model):\n",
    "    cloned_model = clone(model)\n",
    "    cloned_model.class_weight='balanced'\n",
    "    Pred, Prob=original_approach(Xtrain,Ytrain,Xtest,Ytest,cloned_model)\n",
    "    return Pred, Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d08dd726-a249-42a5-a1a3-2f8085aa5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolation_forest(Xtrain,Ytrain,Xtest):\n",
    "    IF = IsolationForest(n_estimators=1000, random_state=1,n_jobs=-1)\n",
    "    IF.fit(Xtrain)\n",
    "    Prob=-IF.decision_function(Xtest)\n",
    "    return Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "89b34bf9-39e6-4e3d-82de-8980ef214286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOF_novelty(Xtrain, Ytrain, Xtest):\n",
    "    LOF = LocalOutlierFactor(n_neighbors=200, n_jobs=-1, novelty=True)\n",
    "    \n",
    "    # ✅ Correction du filtrage de Xtrain\n",
    "    if isinstance(Ytrain, pd.DataFrame):\n",
    "        Xtrain_subset = Xtrain[Ytrain.iloc[:, 0] == 0]\n",
    "    else:  # Si Ytrain est un np.array\n",
    "        Xtrain_subset = Xtrain[Ytrain[:, 0] == 0]\n",
    "    \n",
    "    LOF.fit(Xtrain_subset)\n",
    "    Prob = -LOF.decision_function(Xtest)\n",
    "    return Prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "da106c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def comparaison(Xtrain_1, Xtest_1, Ytrain, Ytest, models):\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "\n",
    "    # Vérification des NaN dans Xtrain_1\n",
    "    if pd.isnull(Xtrain_1).values.any():\n",
    "        print(\"🚨 Attention : Des NaN détectés dans Xtrain ! Remplacement par la médiane.\")\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        Xtrain_1 = imputer.fit_transform(Xtrain_1)\n",
    "        Xtest_1 = imputer.transform(Xtest_1)\n",
    "\n",
    "    # Étape 1 : LOF et Isolation Forest (Placeholder, assure-toi de les définir)\n",
    "    print(\"Local Outlier Factor : Détection de nouveautés\")\n",
    "    # Prob = LOF_novelty(Xtrain_1, Ytrain, Xtest_1)\n",
    "    \n",
    "    print(\"Isolation Forest : Détection d'outliers\")\n",
    "    # Prob = isolation_forest(Xtrain_1, Ytrain, Xtest_1)\n",
    "\n",
    "    # Convertir Ytrain et Ytest en numpy array\n",
    "    Ytrain = np.array(Ytrain)\n",
    "    Ytest = np.array(Ytest)\n",
    "\n",
    "    # Étape 2 : Test des modèles\n",
    "    for name, base_model in models.items():\n",
    "        print(f'***************** {name} *****************')\n",
    "        \n",
    "        if name == 'LR_Norm':\n",
    "            RS = RobustScaler()\n",
    "            Xtrain = RS.fit_transform(Xtrain_1)\n",
    "            Xtest = RS.transform(Xtest_1)\n",
    "        else:\n",
    "            Xtrain, Xtest = Xtrain_1, Xtest_1\n",
    "\n",
    "        # Oversampling et undersampling (Placeholder)\n",
    "        # X_under, Y_under = Tomek(Xtrain, Ytrain)\n",
    "        # X_over, Y_over = Smote(Xtrain, Ytrain)\n",
    "\n",
    "        # Adapter les modèles pour le multi-label\n",
    "        model = OneVsRestClassifier(base_model) if isinstance(base_model, (LogisticRegression, RandomForestClassifier)) else base_model\n",
    "\n",
    "        # Pipeline\n",
    "        pipe = Pipeline([('scaler', RobustScaler()), ('model', model)])\n",
    "\n",
    "        # Définition des hyperparamètres selon le modèle\n",
    "        param_grid = {}\n",
    "        if isinstance(base_model, LogisticRegression):\n",
    "            param_grid = {\n",
    "                'model__estimator__C': [0.1, 1, 10],\n",
    "                'model__estimator__solver': ['liblinear', 'saga']\n",
    "            }\n",
    "        elif isinstance(base_model, RandomForestClassifier):\n",
    "            param_grid = {\n",
    "                'model__estimator__n_estimators': [50, 100, 200],\n",
    "                'model__estimator__max_depth': [None, 10, 20],\n",
    "                'model__estimator__min_samples_split': [2, 5, 10]\n",
    "            }\n",
    "\n",
    "        # GridSearch\n",
    "        if param_grid:\n",
    "            grid_search = GridSearchCV(pipe, param_grid, cv=3, scoring='f1_samples', n_jobs=-1)\n",
    "            grid_search.fit(Xtrain, Ytrain)\n",
    "            current_model = grid_search.best_estimator_\n",
    "            current_score = grid_search.best_score_\n",
    "        else:\n",
    "            current_model = pipe.fit(Xtrain, Ytrain)\n",
    "            current_score = current_model.score(Xtest, Ytest)\n",
    "\n",
    "        # Mise à jour du meilleur modèle\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_model = current_model\n",
    "\n",
    "        # Affichage des scores et courbes PR\n",
    "        print('****** Approche originale ******')\n",
    "        Pred = current_model.predict(Xtest)\n",
    "        Prob = current_model.predict_proba(Xtest) if hasattr(current_model, 'predict_proba') else current_model.decision_function(Xtest)\n",
    "\n",
    "        # Conversion en étiquettes binaires (0 ou 1)\n",
    "        Pred = (Prob >= 0.5).astype(int)\n",
    "        print(classification_report(Ytest, Pred))\n",
    "\n",
    "        # Courbes Précision-Rappel\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for i in range(Ytest.shape[1]):\n",
    "            precision, recall, _ = precision_recall_curve(Ytest[:, i], Prob[:, i])\n",
    "            plt.plot(recall, precision, lw=2, label=f'Label {i}')\n",
    "        \n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(f\"Courbes Précision-Rappel pour {name}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Sauvegarde du meilleur modèle\n",
    "    if best_model:\n",
    "        print(f\"💾 Enregistrement du meilleur modèle avec un score de {best_score}\")\n",
    "        joblib.dump(best_model, 'best_model.pkl')\n",
    "    else:\n",
    "        print(\"❌ Aucun modèle trouvé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a456a222",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db7388a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xtest,Ytrain,Ytest=train_test_split(X_encoded,Y_encoded_df,test_size=0.5,stratify=Y_encoded_df,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4232b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Outlier Factor : Détection de nouveautés\n",
      "Isolation Forest : Détection d'outliers\n",
      "***************** RF *****************\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 187. MiB for an array with shape (15, 1632810) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 136, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 880, in _fit_and_score\n    X_train, y_train = _safe_split(estimator, X, y, train)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 156, in _safe_split\n    X_subset = _safe_indexing(X, indices)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 263, in _safe_indexing\n    return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 44, in _pandas_indexing\n    return X.take(key, axis=axis)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 4133, in take\n    new_data = self._mgr.take(\n               ^^^^^^^^^^^^^^^\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 894, in take\n    return self.reindex_indexer(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 688, in reindex_indexer\n    blk.take_nd(\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 1307, in take_nd\n    new_values = algos.take_nd(\n                 ^^^^^^^^^^^^^^\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py\", line 117, in take_nd\n    return _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py\", line 157, in _take_nd_ndarray\n    out = np.empty(out_shape, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 187. MiB for an array with shape (15, 1632810) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m comparaison(Xtrain,Xtest,Ytrain,Ytest,models)\n",
      "Cell \u001b[1;32mIn[73], line 61\u001b[0m, in \u001b[0;36mcomparaison\u001b[1;34m(Xtrain_1, Xtest_1, Ytrain, Ytest, models)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param_grid:\n\u001b[0;32m     60\u001b[0m     grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipe, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_samples\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m     grid_search\u001b[38;5;241m.\u001b[39mfit(Xtrain, Ytrain)\n\u001b[0;32m     62\u001b[0m     current_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     63\u001b[0m     current_score \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_score_\n",
      "File \u001b[1;32mc:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1572\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    965\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    966\u001b[0m         clone(base_estimator),\n\u001b[0;32m    967\u001b[0m         X,\n\u001b[0;32m    968\u001b[0m         y,\n\u001b[0;32m    969\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    970\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    971\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    972\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    973\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    975\u001b[0m     )\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    979\u001b[0m     )\n\u001b[0;32m    980\u001b[0m )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\joanz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 187. MiB for an array with shape (15, 1632810) and data type float64"
     ]
    }
   ],
   "source": [
    "comparaison(Xtrain,Xtest,Ytrain,Ytest,models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
